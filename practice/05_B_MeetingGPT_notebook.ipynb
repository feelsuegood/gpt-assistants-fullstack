{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    # \"-y\": overwrite audio file\n",
    "    command = [\"ffmpeg\", \"-i\", video_path, \"-vn\", audio_path, \"-y\"]\n",
    "    subprocess.run(command)\n",
    "\n",
    "\n",
    "def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):\n",
    "    # create folder if not exists\n",
    "    os.makedirs(chunks_folder, exist_ok=True)\n",
    "\n",
    "    track = AudioSegment.from_mp3(audio_path)\n",
    "    chunk_leng = chunk_size * 60 * 1000\n",
    "    chunks = math.ceil(len(track) / chunk_leng)\n",
    "\n",
    "    for i in range(chunks):\n",
    "        # print(i)\n",
    "        start_time = i * chunk_leng\n",
    "        end_time = (i + 1) * chunk_leng\n",
    "        # print(f\"start: {start_time}, end: {end_time}\")\n",
    "        chunk = track[start_time:end_time]\n",
    "        chunk.export(f\"{chunks_folder}/chunk_{i}.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_audio_from_video(\"../files/mcp.mp4\", \"../files/mcp.mp3\")\n",
    "cut_audio_in_chunks(\"../files/mcp.mp3\", 1, \"../files/chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import glob\n",
    "\n",
    "openai.api_type = \"openai\"\n",
    "\n",
    "\n",
    "def transcribe_chunks(chunks_folder, destination):\n",
    "    files = glob.glob(f\"{chunks_folder}/*.mp3\")\n",
    "    # final_transcript = \"\"\n",
    "    for file in files:\n",
    "        # \"r\": read, \"b\" binary, \"a\": append\n",
    "        # with open(file, \"rb\") as audio_file:\n",
    "        with open(file, \"rb\") as audio_file, open(destination, \"a\") as text_file:\n",
    "            transcript = openai.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file,\n",
    "                language=\"ko\",\n",
    "            )\n",
    "            text_file.write(transcript.text)\n",
    "\n",
    "\n",
    "transcribe_chunks(\"../files/chunks\", \"../files/transcript.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "def translate_to_english(korean_text_path, english_text_path):\n",
    "    with open(korean_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        korean_text = f.read()\n",
    "\n",
    "    # split by sentence\n",
    "    sentences = korean_text.split(\". \")\n",
    "    english_sentences = []\n",
    "\n",
    "    for s in sentences:\n",
    "        if s.strip():\n",
    "            try:\n",
    "                translated = GoogleTranslator(source=\"ko\", target=\"en\").translate(s)\n",
    "                english_sentences.append(translated)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ translation failed: {s[:30]}... -> {e}\")\n",
    "                english_sentences.append(\"[translation failed]\")\n",
    "\n",
    "    final_text = \". \".join(english_sentences)\n",
    "\n",
    "    # save text\n",
    "    with open(english_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final_text)\n",
    "\n",
    "    print(f\"âœ… translated text saved: {english_text_path}\")\n",
    "    return english_text_path\n",
    "\n",
    "\n",
    "translate_to_english(\"../files/transcript.txt\", \"../files/translated_english.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      "ðŸŽ™ Generating (1/74) â†’ It's a trendy to know MCP to use AI these days, bu...\n",
      " > Text splitted to sentences.\n",
      "[\"It's a trendy to know MCP to use AI these days, but it's a bit of a strange analogy that it's hard and I can't understand it.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed\n",
      "[nltk_data]     (_ssl.c:1002)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 3.2039549350738525\n",
      " > Real-time factor: 0.28507007520812533\n",
      "ðŸŽ™ Generating (2/74) â†’ It's a strange parable....\n",
      " > Text splitted to sentences.\n",
      "[\"It's a strange parable.\"]\n",
      " > Processing time: 0.590986967086792\n",
      " > Real-time factor: 0.2519189341220183\n",
      "ðŸŽ™ Generating (3/74) â†’ Groccortlolor is a crocodile that can be blown and...\n",
      " > Text splitted to sentences.\n",
      "['Groccortlolor is a crocodile that can be blown and bombed, but it can be transformed into ground mode.']\n",
      " > Processing time: 2.223515272140503\n",
      " > Real-time factor: 0.290150741825455\n",
      "ðŸŽ™ Generating (4/74) â†’ Brer Bre Pa Tabim can attack opponents with wooden...\n",
      " > Text splitted to sentences.\n",
      "['Brer Bre Pa Tabim can attack opponents with wooden roots.']\n",
      " > Processing time: 1.2421321868896484\n",
      " > Real-time factor: 0.28002836905893946\n",
      "ðŸŽ™ Generating (5/74) â†’ Why did you talk about the same thing when you're ...\n",
      " > Text splitted to sentences.\n",
      "[\"Why did you talk about the same thing when you're developing AI services these days, so it's a big deal to make an agent and give AI a power to make a battle.\"]\n",
      " > Processing time: 3.1421868801116943\n",
      " > Real-time factor: 0.29384042167021296\n",
      "ðŸŽ™ Generating (6/74) â†’ It is named Dralarero and is released like this, a...\n",
      " > Text splitted to sentences.\n",
      "['It is named Dralarero and is released like this, and it is being invested in a large sum.', '.']\n",
      "   > Decoder stopped with `max_decoder_steps` 10000\n",
      " > Processing time: 41.53507399559021\n",
      " > Real-time factor: 0.29733174652322825\n",
      "ðŸŽ™ Generating (7/74) â†’ But there is a problem....\n",
      " > Text splitted to sentences.\n",
      "['But there is a problem.']\n",
      " > Processing time: 0.396960973739624\n",
      " > Real-time factor: 0.21361258958802004\n",
      "ðŸŽ™ Generating (8/74) â†’ Of course, when creating an AI agent, you have to ...\n",
      " > Text splitted to sentences.\n",
      "['Of course, when creating an AI agent, you have to make a code, but if you add one by one, the code will be open quickly.', '.', 'Because each person weaves the code differently, it is hard to get help from others on the Internet.', '.']\n",
      "   > Decoder stopped with `max_decoder_steps` 10000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from TTS.api import TTS\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def generate_english_speech_from_file(english_text_path, english_audio_output_path):\n",
    "\n",
    "    with open(english_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        english_text = f.read()\n",
    "        # * \"Iâ€™m happy.\" â†’ \"Im happy\"\n",
    "        # cleaned_english_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", english_text)\n",
    "        # * \"Iâ€™m happy ðŸ˜Š\" â†’ \"I'm happy\"\n",
    "        cleaned_english_text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", english_text)\n",
    "\n",
    "    tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False)\n",
    "    # * fast pitch is faster but less natural\n",
    "    # tts = TTS(model_name=\"tts_models/en/ljspeech/fast_pitch\", progress_bar=False)\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    sentences = sent_tokenize(cleaned_english_text)\n",
    "    total = len(sentences)\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(sentence.strip()) < 5:\n",
    "            print(f\"âš ï¸ Skipping ({i+1}/{total}): too short.\")\n",
    "            continue\n",
    "        temp_file = f\"../files/temp/temp_{i+1}.wav\"\n",
    "        print(f\"ðŸŽ™ Generating ({i+1}/{total}) â†’ {sentence[:50]}...\")\n",
    "        tts.tts_to_file(text=sentence.strip(), file_path=temp_file)\n",
    "        chunk = AudioSegment.from_wav(temp_file)\n",
    "        combined += chunk\n",
    "\n",
    "    combined.export(english_audio_output_path, format=\"wav\")\n",
    "\n",
    "    print(f\"âœ… audio file generated: {english_audio_output_path}\")\n",
    "    return english_audio_output_path\n",
    "\n",
    "\n",
    "generate_english_speech_from_file(\n",
    "    \"../files/translated_english.txt\", \"../files/english_audio.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_audio_with_video(original_video, new_audio, output_video):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-y\",\n",
    "        \"-i\",\n",
    "        original_video,\n",
    "        \"-i\",\n",
    "        new_audio,\n",
    "        \"-c:v\",\n",
    "        \"copy\",\n",
    "        \"-map\",\n",
    "        \"0:v:0\",\n",
    "        \"-map\",\n",
    "        \"1:a:0\",\n",
    "        \"-shortest\",\n",
    "        output_video,\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "    return output_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_audio_with_video(\n",
    "    \"../files/mcp.mp4\", \"../files/english_audio.wav\", \"../files/mcp_dubbed.mp4\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
