{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from pydub import AudioSegment\n",
    "import math\n",
    "\n",
    "\n",
    "def extract_audio_from_video(video_path, audio_path):\n",
    "    # \"-y\": overwrite audio file\n",
    "    command = [\"ffmpeg\", \"-i\", video_path, \"-vn\", audio_path, \"-y\"]\n",
    "    subprocess.run(command)\n",
    "\n",
    "\n",
    "def cut_audio_in_chunks(audio_path, chunk_size, chunks_folder):\n",
    "    # create folder if not exists\n",
    "    os.makedirs(chunks_folder, exist_ok=True)\n",
    "\n",
    "    track = AudioSegment.from_mp3(audio_path)\n",
    "    chunk_leng = chunk_size * 60 * 1000\n",
    "    chunks = math.ceil(len(track) / chunk_leng)\n",
    "\n",
    "    for i in range(chunks):\n",
    "        # print(i)\n",
    "        start_time = i * chunk_leng\n",
    "        end_time = (i + 1) * chunk_leng\n",
    "        # print(f\"start: {start_time}, end: {end_time}\")\n",
    "        chunk = track[start_time:end_time]\n",
    "        chunk.export(f\"{chunks_folder}/chunk_{i}.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_audio_from_video(\"../files/mcp.mp4\", \"../files/mcp.mp3\")\n",
    "cut_audio_in_chunks(\"../files/mcp.mp3\", 1, \"../files/chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import glob\n",
    "\n",
    "openai.api_type = \"openai\"\n",
    "\n",
    "\n",
    "def transcribe_chunks(chunks_folder, destination):\n",
    "    files = glob.glob(f\"{chunks_folder}/*.mp3\")\n",
    "    # final_transcript = \"\"\n",
    "    for file in files:\n",
    "        # \"r\": read, \"b\" binary, \"a\": append\n",
    "        # with open(file, \"rb\") as audio_file:\n",
    "        with open(file, \"rb\") as audio_file, open(destination, \"a\") as text_file:\n",
    "            transcript = openai.audio.transcriptions.create(\n",
    "                model=\"whisper-1\",\n",
    "                file=audio_file,\n",
    "                language=\"ko\",\n",
    "            )\n",
    "            text_file.write(transcript.text)\n",
    "\n",
    "\n",
    "transcribe_chunks(\"../files/chunks\", \"../files/transcript.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "def translate_to_english(korean_text_path, english_text_path):\n",
    "    with open(korean_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        korean_text = f.read()\n",
    "\n",
    "    # split by sentence\n",
    "    sentences = korean_text.split(\". \")\n",
    "    english_sentences = []\n",
    "\n",
    "    for s in sentences:\n",
    "        if s.strip():\n",
    "            try:\n",
    "                translated = GoogleTranslator(source=\"ko\", target=\"en\").translate(s)\n",
    "                english_sentences.append(translated)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ translation failed: {s[:30]}... -> {e}\")\n",
    "                english_sentences.append(\"[translation failed]\")\n",
    "\n",
    "    final_text = \". \".join(english_sentences)\n",
    "\n",
    "    # save text\n",
    "    with open(english_text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final_text)\n",
    "\n",
    "    print(f\"✅ translated text saved: {english_text_path}\")\n",
    "    return english_text_path\n",
    "\n",
    "\n",
    "translate_to_english(\"../files/transcript.txt\", \"../files/translated_english.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from TTS.api import TTS\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def generate_english_speech_from_file(english_text_path, english_audio_output_path):\n",
    "\n",
    "    with open(english_text_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        english_text = f.read()\n",
    "        # * \"I’m happy.\" → \"Im happy\"\n",
    "        # cleaned_english_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", english_text)\n",
    "        # * \"I’m happy 😊\" → \"I'm happy\"\n",
    "        cleaned_english_text = re.sub(r\"[^\\x00-\\x7F]+\", \"\", english_text)\n",
    "\n",
    "    tts = TTS(model_name=\"tts_models/en/ljspeech/tacotron2-DDC\", progress_bar=False)\n",
    "    # * fast pitch is faster but less natural\n",
    "    # tts = TTS(model_name=\"tts_models/en/ljspeech/fast_pitch\", progress_bar=False)\n",
    "\n",
    "    nltk.download(\"punkt\")\n",
    "\n",
    "    sentences = sent_tokenize(cleaned_english_text)\n",
    "    total = len(sentences)\n",
    "    combined = AudioSegment.silent(duration=0)\n",
    "\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if len(sentence.strip()) < 5:\n",
    "            print(f\"⚠️ Skipping ({i+1}/{total}): too short.\")\n",
    "            continue\n",
    "        temp_file = f\"../files/temp/temp_{i+1}.wav\"\n",
    "        print(f\"🎙 Generating ({i+1}/{total}) → {sentence[:50]}...\")\n",
    "        tts.tts_to_file(text=sentence.strip(), file_path=temp_file)\n",
    "        chunk = AudioSegment.from_wav(temp_file)\n",
    "        combined += chunk\n",
    "\n",
    "    combined.export(english_audio_output_path, format=\"wav\")\n",
    "\n",
    "    print(f\"✅ audio file generated: {english_audio_output_path}\")\n",
    "    return english_audio_output_path\n",
    "\n",
    "\n",
    "generate_english_speech_from_file(\n",
    "    \"../files/translated_english.txt\", \"../files/english_audio.wav\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_audio_with_video(original_video, new_audio, output_video):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-y\",\n",
    "        \"-i\",\n",
    "        original_video,\n",
    "        \"-i\",\n",
    "        new_audio,\n",
    "        \"-c:v\",\n",
    "        \"copy\",\n",
    "        \"-map\",\n",
    "        \"0:v:0\",\n",
    "        \"-map\",\n",
    "        \"1:a:0\",\n",
    "        \"-shortest\",\n",
    "        output_video,\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "    return output_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_audio_with_video(\n",
    "    \"../files/mcp.mp4\", \"../files/english_audio.wav\", \"../files/mcp_dubbed.mp4\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
